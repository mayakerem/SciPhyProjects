{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7: Neural networks\n",
    "\n",
    "## [Scientific Computing with Python](http://scicompy.yoavram.com)\n",
    "## Yoav Ram\n",
    "\n",
    "# General instructions\n",
    "\n",
    "1. Do not remove any text or code cells; do not leave redundent print messages.\n",
    "1. When instructed to implement a function, use the given function names and parameters lists; failure to do so may cause test functions to fail during grading.\n",
    "1. When instructed to generate a plot, make sure that the plot is clear, that axes are propely labeled, and that the notebook is saved with the plot inline, so that the grader can see the plot without running the code. Make sure that you re-generate the plot if you changed the code!\n",
    "1. Code lines with a triple comment `###` should not be removed or modified, they are used for automatic grading.\n",
    "1. Note that there are 3 exercises and the last cell in the notebook says **end of assignment**; if you are missing anything please download the origianl file from the course website.\n",
    "1. Your code should run within a reasonable time (a few minutes) and you should use idioms learned in class, e.g. array opreations, numba, multiprocessig.\n",
    "1. Questions regarding the exercises should be posted to the course forum at the designated group (i.e. \"assignment7\"). You can post questions anonymously. You can also visit the Office Hours, but please do not email the course staff with questions about the exercise.\n",
    "1. Intructions for submitting the exercise are on the [course website](https://scicompy.yoavram.com/assignments).\n",
    "\n",
    "# Cloud computing with GPU\n",
    "\n",
    "**In Ex 2 and Ex 3 there is a benefit in running on a computer with a GPU. There are two ways to do that:**\n",
    "1. Use [**Google Colaboratory**](http://colab.research.google.com) (free.) Colaboratory provides free GPU usage for 12 hours inside a Jupyter notebook. Once you are inside colaboratory, upload this notebook, and change the runtime to Python 3 + GPU. Don't forget to download your notebook when you finish (from the File menu), although it will be saved in your Google Docs.\n",
    "\n",
    "1. Use **AWS Educate**. You can sign up for an AWS Educate account without a credict card, and get some free credits to use for AWS services. You can get an additional \\$50 for this course - just ask for it on Piazza and allow for 24 hrs.\n",
    "[Instructions are provided](https://aws.amazon.com/blogs/machine-learning/get-started-with-deep-learning-using-the-aws-deep-learning-ami/) for setting up a deep learning environment with conda, jupyter, and keras. Note that when you go over the credit limit (\\$50) your account will be suspended and you will not have access to your work, so plan accordingly. Some GPU-enabled instances estimated  costs are: p2.xlarge (Tesla K80) 22 USD/day; p3.2xlarge (2 x Tesla v100) 75 USD/day; p3.16xlarge (8 x Tesla v100) 600 USD/day. At this stage you should probably take the cheapest one.\n",
    "\n",
    "It may be worth the effort: epochs on the dataset used in Ex 2 and Ex 3 take about 15 secs on colaboratory, compared to 40 secs on my laptop. \n",
    "But it is not mandatory to work with a GPU, you can use your local CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "import scipy.stats\n",
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Flatten\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# this code is required for Ex 1\n",
    "def display_image(im):\n",
    "    plt.imshow(im.reshape((28, 28)), cmap='gray_r')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    \n",
    "def fully_connected(ninputs, noutputs):\n",
    "    boundary = np.sqrt(6 / (ninputs + noutputs))\n",
    "    return np.random.uniform(-boundary, boundary, size=(ninputs, noutputs))\n",
    "\n",
    "def softmax(x):\n",
    "    expx = np.exp(x - x.max(axis=1, keepdims=True))\n",
    "    return expx / expx.sum(axis=1, keepdims=True)\n",
    "\n",
    "def accuracy(Yhat, Y):\n",
    "    return (Y.argmax(axis=1) == Yhat.argmax(axis=1)).mean()\n",
    "\n",
    "def cross_entropy(Yhat, Y):\n",
    "    ylogy = Y * np.log(Yhat)\n",
    "    return -ylogy.sum()\n",
    "\n",
    "def ReLU(X):\n",
    "    return np.maximum(X, 0)\n",
    "\n",
    "def dReLU(X):\n",
    "    return (X > 0).astype(float)\n",
    "\n",
    "def drop(X, keep_prob=1):\n",
    "    if keep_prob < 1:\n",
    "        X = X.copy() # we don't want to change X\n",
    "        keeps = np.random.rand(X.shape[1]) < keep_prob\n",
    "        # X.shape is (nsamples, nfeatures)\n",
    "        X[:, ~keeps] = 0 # ignore\n",
    "        X[:, keeps] *= (1/keep_prob) # normalize\n",
    "    return X\n",
    "\n",
    "def predict(Ws, X):\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape((1, -1))\n",
    "    return feed_forward(Ws, X, keep_prob=1)[-1]\n",
    "\n",
    "def display_prediction(idx):\n",
    "    prediction = predict(Ws, X_test[idx, :]).argmax()\n",
    "    print(prediction)\n",
    "    return display_image(X_test[idx])\n",
    "\n",
    "def loss(Ws, X, Y):\n",
    "    Yhat = predict(Ws, X)\n",
    "    return cross_entropy(Yhat, Y)\n",
    "\n",
    "def gradient_check(Ws, X, Y, Δ=1e-5):\n",
    "    dWs = back_propagation(Ws, X, Y, keep_prob=1)\n",
    "    Ws_ = [W.copy() for W in Ws]\n",
    "\n",
    "    for i, (W_, dW_) in enumerate(zip(Ws_, dWs)):\n",
    "        print('W{}'.format(i+1))\n",
    "        for i in range(W_.shape[0]):\n",
    "            for j in range(W_.shape[1]):\n",
    "                dw = dW_[i, j]\n",
    "                W_[i,j] += Δ\n",
    "                loss1 = loss(Ws_, X, Y)\n",
    "                W_[i,j] -= 2*Δ\n",
    "                loss2 = loss(Ws_, X, Y)\n",
    "                W_[i,j] += Δ\n",
    "                dw_ = (loss1 - loss2) / (2 * Δ)\n",
    "                rel_error = abs(dw - dw_) / abs(dw + dw_)\n",
    "                if not np.isclose(dw_, dw):\n",
    "                    print(i, j, dw, dw_, rel_error)\n",
    "                    \n",
    "def average(prev, curr, β):\n",
    "    return [\n",
    "        β * p + (1 - β) * c\n",
    "        for p, c\n",
    "        in zip(prev, curr)\n",
    "    ]\n",
    "    \n",
    "class AdamOptimizer:\n",
    "    def __init__(self, α=0.001, β1=0.9, β2=0.999, ϵ=1e-8):\n",
    "        self.α = α\n",
    "        self.β1 = β1\n",
    "        self.β2 = β2\n",
    "        self.ϵ = ϵ\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "\n",
    "    def send(self, gradients):\n",
    "        if self.m is None:\n",
    "            self.m = [0] * len(gradients)\n",
    "        if self.v is None:\n",
    "            self.v = [0] * len(gradients)\n",
    "\n",
    "        self.t += 1\n",
    "        αt = self.α * np.sqrt(1 - self.β2**self.t) / (1 - self.β1**self.t)\n",
    "        self.m = average(self.m, gradients, self.β1)        \n",
    "        self.v = average(self.v, (g*g for g in gradients), self.β2)\n",
    "\n",
    "        updates = [-αt * mi / (np.sqrt(vi) + self.ϵ) for mi, vi in zip(self.m, self.v)]\n",
    "        for upd in updates:\n",
    "            assert np.isfinite(upd).all()\n",
    "        return updates\n",
    "    \n",
    "def trainer(Ws, X, Y, optimizer, batch_size=50, keep_prob=1):    \n",
    "    nsamples = X.shape[0]\n",
    "    batch = 0\n",
    "    while True:\n",
    "        # get next batch\n",
    "        start = (batch * batch_size) % nsamples\n",
    "        stop = start + batch_size\n",
    "        batch_idx = range(start, stop)\n",
    "        X_, Y_ = X[batch_idx, :], Y[batch_idx, :]\n",
    "        \n",
    "        gradients = back_propagation(Ws, X_, Y_, keep_prob=keep_prob) # calculate gradients\n",
    "        \n",
    "        ΔWs = optimizer.send(gradients) # calculate updates\n",
    "        \n",
    "        for W, ΔW in zip(Ws, ΔWs): # apply updates\n",
    "            W += ΔW\n",
    "            \n",
    "        batch += 1\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 1\n",
    "\n",
    "Let's load the data for MNIST digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "(X_train, Y_train), (X_test, Y_test) = keras.datasets.mnist.load_data()\n",
    "nsamples, width, height = X_train.shape\n",
    "nfeatures = width * height\n",
    "X_train = X_train.reshape(nsamples, nfeatures)\n",
    "X_test = X_test.reshape(-1, nfeatures)\n",
    "Y_train = keras.utils.to_categorical(Y_train)\n",
    "Y_test = keras.utils.to_categorical(Y_test)\n",
    "ncats = Y_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  51 159 253 159  50   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  48 238 252 252 252 237   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  54 227 253 252 239 233 252  57   6   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0  10  60 224 252 253 252 202  84 252\n",
      " 253 122   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0 163 252 252 252 253 252 252  96 189 253 167   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  51 238 253 253 190 114 253 228\n",
      "  47  79 255 168   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0  48 238 252 252 179  12  75 121  21   0   0 253 243  50   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  38 165 253 233 208  84   0   0\n",
      "   0   0   0   0 253 252 165   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   7 178 252 240  71  19  28   0   0   0   0   0   0 253 252 195   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0  57 252 252  63   0   0   0\n",
      "   0   0   0   0   0   0 253 252 195   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0 198 253 190   0   0   0   0   0   0   0   0   0   0 255 253\n",
      " 196   0   0   0   0   0   0   0   0   0   0   0  76 246 252 112   0   0\n",
      "   0   0   0   0   0   0   0   0 253 252 148   0   0   0   0   0   0   0\n",
      "   0   0   0   0  85 252 230  25   0   0   0   0   0   0   0   0   7 135\n",
      " 253 186  12   0   0   0   0   0   0   0   0   0   0   0  85 252 223   0\n",
      "   0   0   0   0   0   0   0   7 131 252 225  71   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0  85 252 145   0   0   0   0   0   0   0  48 165\n",
      " 252 173   0   0   0   0   0   0   0   0   0   0   0   0   0   0  86 253\n",
      " 225   0   0   0   0   0   0 114 238 253 162   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  85 252 249 146  48  29  85 178 225 253\n",
      " 223 167  56   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "  85 252 252 252 229 215 252 252 252 196 130   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0  28 199 252 252 253 252 252 233\n",
      " 145   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0  25 128 252 253 252 141  37   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "# print(keras.datasets.mnist.load_data())\n",
    "print(X_train[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement `feed_forward` and `back_propagation` from the FFN lecture using a `for` loop so that they work for any number of hidden layers.**\n",
    "\n",
    "Notes: \n",
    "- when implementing `back_propagation`, you can use the `gradient_check` function (from `A5.py`) to test that it works as expected.\n",
    "- please keep the functions signatures as supplied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [1., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)) (array([[0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0],\n",
      "       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8), array([[0., 0., 0., ..., 1., 0., 0.],\n",
      "       [0., 0., 1., ..., 0., 0., 0.],\n",
      "       [0., 1., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32))\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print((X_train, Y_train), (X_test, Y_test))\n",
    "print(ncats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(Ws, X, keep_prob=1): ###\n",
    "    network = [X]\n",
    "    W_hidden = Ws[:-1] # All but last Ws\n",
    "    W_final = Ws[-1] # Last Ws\n",
    "    \n",
    "    for i, W in enumerate(W_hidden): # For all Ws of the hiddle layer\n",
    "        # For each layer\n",
    "        X = X @ W              # Computing dot product\n",
    "        X = drop(X,keep_prob)\n",
    "        X = ReLU(X)\n",
    "        network.append(X)      # Add layer\n",
    "  \n",
    "    # For the last layer   \n",
    "    X = X @ W_final   \n",
    "#     X = drop(X, keep_prob=keep_prob)\n",
    "    X = softmax(X) \n",
    "    network.append(X)      # Add layer\n",
    "\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(Ws, X, Y, keep_prob=1): ###\n",
    "    layers = feed_forward(Ws, X, keep_prob=keep_prob) # X1, Z1, X2, Z2, Yhat\n",
    "    gradients = []\n",
    "\n",
    "    # In backprop we start with the last layer\n",
    "    layer = layers.pop() \n",
    "#     print('Layers.pop {}\\n'.format(layer.shape))\n",
    "#     print('Y {} \\n'.format(Y.shape))\n",
    "\n",
    "    delta = layer - Y # we dont want to consider target values\n",
    "#     print(Y)\n",
    "\n",
    "    Ws_num = len(Ws)+1 # We want to go over all the layer weights +1\n",
    "\n",
    "    for i in range(1,Ws_num):\n",
    "        # We start by getting the gradient\n",
    "        Z = layers.pop() # Current layer (will iterate as we iterate over range)\n",
    "        Z_transpose = Z.T\n",
    "        gradients.append(Z_transpose @ delta)  #adding the gradients of Z\n",
    "\n",
    "        if i == (Ws_num - 1):\n",
    "            break\n",
    "       \n",
    "        # Now we are calculating the cost\n",
    "\n",
    "        # Take the ith layer from the back (because in backprop we start with the last layer)\n",
    "        W_transpose = Ws[-i].T\n",
    "        delta = (delta @ W_transpose) * dReLU(Z)  # δ = δ • W * ReLU(Z)\n",
    "    # change order\n",
    "    gradients.reverse()\n",
    "    # sanity checks (From class)\n",
    "    assert len(gradients) == len(Ws), (len(gradients), len(Ws))\n",
    "    for dW, W in zip(gradients, Ws):\n",
    "        assert dW.shape == W.shape, (dW.shape, W.shape)\n",
    "    return gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now **train the FFN model** with 2 or more hidden layers, and print its accuracy on the test set.\n",
    "\n",
    "I was able to achieve 0.93 test accuracy with about 7-8 seconds of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "784"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "### change the ?s\n",
    "# nhidden =784\n",
    "# l1, l2 = 50, 25\n",
    "\n",
    "W1 = fully_connected(784, 50)\n",
    "W2 = fully_connected(50, 25)\n",
    "W3 = fully_connected(25, 10)\n",
    "Ws = [W1, W2, W3]\n",
    "\n",
    "batch_size = 100\n",
    "train = trainer(Ws, X_train, Y_train, batch_size=batch_size, optimizer=AdamOptimizer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0503\n"
     ]
    }
   ],
   "source": [
    "acc = accuracy(predict(Ws, X_test), Y_test)\n",
    "print(\"Accuracy: {:.4f}\".format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (6000): 0.9453\n"
     ]
    }
   ],
   "source": [
    "### change the ?s\n",
    "epochs = 10\n",
    "for batch in train:\n",
    "    if batch == epochs * nsamples // batch_size: break\n",
    "\n",
    "acc = accuracy(predict(Ws, X_test), Y_test)\n",
    "print(\"Accuracy ({:d}): {:.4f}\".format(batch, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ex 2\n",
    "\n",
    "## Keras\n",
    "\n",
    "[Keras](https://keras.io/) is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, [CNTK](https://docs.microsoft.com/en-us/cognitive-toolkit/) (MS deep learning package), or [Theano](http://www.deeplearning.net/software/theano/).\n",
    "\n",
    "The main benefit is that it *allows for easy and fast prototyping*.\n",
    "\n",
    "Since version 1.4 of TensorFlow, Keras is installed with TensorFlow as a high-level interface to TensorFlow.\n",
    "\n",
    "## Fashion-MNIST datasetset\n",
    "\n",
    "The Fashion-MNIST dataset contains 60,000 28x28 grayscale images of 10 fashion categories, along with a test set of 10,000 images. This dataset can be used as a drop-in replacement for MNIST. The class labels are:\n",
    "\n",
    "\n",
    "| Label |\tDescription|\n",
    "|---|------------------|\n",
    "| 0 |\tT-shirt/top    |\n",
    "| 1 |\tTrouser        |\n",
    "| 2 |\tPullover       |\n",
    "| 3 |\tDress          |\n",
    "| 4 |\tCoat           |\n",
    "| 5 |\tSandal         |\n",
    "| 6 |\tShirt          |\n",
    "| 7 |\tSneaker        |\n",
    "| 8 |\tBag            |\n",
    "| 9 |\tAnkle boot     |\n",
    "\n",
    "See more information on this dataset in the [keras docs](https://keras.io/datasets/).\n",
    "\n",
    "In this exercise we will use Keras to train a neural network on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras 2.3.1\n",
      "GPU: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "try:\n",
    "    import keras\n",
    "except ModuleNotFoundError:\n",
    "    from tensorflow import keras\n",
    "print('Keras', keras.__version__)\n",
    "print('GPU:', tf.test.is_gpu_available())\n",
    "print(tf.test.gpu_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the images to a float32 between 0 and 1 and reshape to 28x28x1 (only one channel for black and white) because 2D convolutions expect 3D images (3rd dimension is channel or image).\n",
    "\n",
    "We also need to one-hot encode the labels, and to scale the data from 0-255 to 0-1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "x_train = x_train.reshape((-1, 28, 28, 1))\n",
    "x_test = x_test.reshape((-1, 28, 28, 1))\n",
    "num_classes = len(np.unique(y_train))\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 144x144 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "###\n",
    "i = np.random.randint(0, x_train.shape[0])\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.imshow(x_train[i, :, :, 0], cmap='gray_r')\n",
    "plt.xticks([]); plt.yticks([])\n",
    "print(y_train[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras\n",
    "\n",
    "A Keras model can be built by defining the model layers, then giving them to the `Sequential` model object.\n",
    "You could also first create the model object and the use its `add` method to add layers.\n",
    "\n",
    "A `Dense` layer uses matrix muliplication. We need to provide it with the number of outputs, in this case the number of categories, and the activation function, in this case the softmax function.\n",
    "\n",
    "A `Flatten` layer just flatten the input data - turns it into a 1D array.\n",
    "The first layer always has to be told the shape of the input (`X`) using `input_shape`.\n",
    "\n",
    "```python\n",
    "layers = [\n",
    "    keras.layers.LayerName(arguments),\n",
    "    ...\n",
    "]\n",
    "model = keras.models.Sequential(layers)\n",
    "```\n",
    "\n",
    "After defining the network, we can print a summary using `model.summary()`.\n",
    "Then, we need to compile the Keras model (which creates the underlying TensorFlow network) and configuring it for training.\n",
    "At this stage we can choose the [loss function](https://keras.io/api/losses/), the [optimizer](https://keras.io/api/optimizers/), and additional [metrics](https://keras.io/api/metrics/) we with to collect, such as accuracy.\n",
    "\n",
    "```python\n",
    "model.compile(\n",
    "    loss=...,\n",
    "    optimizer=...,\n",
    "    metrics=[...]\n",
    ")\n",
    "```\n",
    "\n",
    "After compiling the network, we are ready to train it by giving it the train and test datasets:\n",
    "Training is done by calling the `fit` method of the model object\n",
    "```python\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    batch_size=...,\n",
    "    epochs=...,\n",
    "    validation_data=(x_test, y_test)\n",
    ")\n",
    "```\n",
    "\n",
    "The `fit` method retuns a `history` object that can be used to plot the accuracy and loss over time (see plot below).\n",
    "\n",
    "**Build, train, and score a neural network**. \n",
    "\n",
    "You can use a feed forward network, or you can use a more sophisticated approach with a convolutional neural network (CNN). Or try both.\n",
    "As long as you reach a high enough accuracy (>80%) it is fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initializing the model\n",
    "# model = Sequential()\n",
    "# input_shape = (28, 28, 1)\n",
    "\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=input_shape),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_29\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_5 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_42 (Dense)             (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 101,770\n",
      "Trainable params: 101,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#To visualize neural network\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/64\n",
      "60000/60000 [==============================] - 40s 674us/step - loss: 0.4825 - accuracy: 0.8278 - val_loss: 0.4130 - val_accuracy: 0.8525\n",
      "Epoch 2/64\n",
      "60000/60000 [==============================] - 40s 659us/step - loss: 0.3679 - accuracy: 0.8658 - val_loss: 0.3914 - val_accuracy: 0.8609\n",
      "Epoch 3/64\n",
      "60000/60000 [==============================] - 38s 640us/step - loss: 0.3315 - accuracy: 0.8783 - val_loss: 0.3711 - val_accuracy: 0.8642\n",
      "Epoch 4/64\n",
      "60000/60000 [==============================] - 39s 658us/step - loss: 0.3118 - accuracy: 0.8858 - val_loss: 0.3583 - val_accuracy: 0.8730\n",
      "Epoch 5/64\n",
      "60000/60000 [==============================] - 39s 656us/step - loss: 0.2963 - accuracy: 0.8910 - val_loss: 0.3640 - val_accuracy: 0.8753\n",
      "Epoch 6/64\n",
      "60000/60000 [==============================] - 37s 609us/step - loss: 0.2808 - accuracy: 0.8959 - val_loss: 0.3439 - val_accuracy: 0.8766\n",
      "Epoch 7/64\n",
      "60000/60000 [==============================] - 33s 542us/step - loss: 0.2695 - accuracy: 0.8998 - val_loss: 0.3811 - val_accuracy: 0.8718\n",
      "Epoch 8/64\n",
      "60000/60000 [==============================] - 33s 545us/step - loss: 0.2581 - accuracy: 0.9044 - val_loss: 0.3435 - val_accuracy: 0.8798\n",
      "Epoch 9/64\n",
      "60000/60000 [==============================] - 33s 550us/step - loss: 0.2495 - accuracy: 0.9069 - val_loss: 0.3593 - val_accuracy: 0.8787\n",
      "Epoch 10/64\n",
      "60000/60000 [==============================] - 32s 540us/step - loss: 0.2411 - accuracy: 0.9092 - val_loss: 0.3418 - val_accuracy: 0.8850\n",
      "Epoch 11/64\n",
      "60000/60000 [==============================] - 32s 538us/step - loss: 0.2362 - accuracy: 0.9115 - val_loss: 0.3570 - val_accuracy: 0.8816\n",
      "Epoch 12/64\n",
      "60000/60000 [==============================] - 32s 539us/step - loss: 0.2285 - accuracy: 0.9145 - val_loss: 0.3362 - val_accuracy: 0.8891\n",
      "Epoch 13/64\n",
      "60000/60000 [==============================] - 32s 540us/step - loss: 0.2236 - accuracy: 0.9168 - val_loss: 0.3716 - val_accuracy: 0.8806\n",
      "Epoch 14/64\n",
      "60000/60000 [==============================] - 34s 560us/step - loss: 0.2169 - accuracy: 0.9188 - val_loss: 0.3743 - val_accuracy: 0.8829\n",
      "Epoch 15/64\n",
      "60000/60000 [==============================] - 33s 542us/step - loss: 0.2114 - accuracy: 0.9199 - val_loss: 0.3700 - val_accuracy: 0.8853\n",
      "Epoch 16/64\n",
      "22620/60000 [==========>...................] - ETA: 29s - loss: 0.2076 - accuracy: 0.9216"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-351-edc3d47f615e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m                    )\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3292\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3293\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3294\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, \n",
    "                    y_train,\n",
    "                    epochs=64, \n",
    "                    batch_size=10,\n",
    "                    validation_data=(x_test, y_test)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### score trained model\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot accuracy over training time\n",
    "plt.plot(history['accuracy'], label='train')\n",
    "plt.plot(history['val_accuracy'], label='test')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 33us/step\n",
      "Test loss: 0.36533783659935\n",
      "Test accuracy: 0.8720999956130981\n"
     ]
    }
   ],
   "source": [
    "### score trained model\n",
    "loss, acc = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### plot accuracy over training time\n",
    "plt.plot(history['accuracy'], label='train')\n",
    "plt.plot(history['val_accuracy'], label='test')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**end of assignment**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SciPhy",
   "language": "python",
   "name": "sciphy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
